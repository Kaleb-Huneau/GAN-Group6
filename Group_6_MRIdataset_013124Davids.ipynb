{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaleb-Huneau/GAN-Group6/blob/Reduced-images/Group_6_MRIdataset_013124Davids.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section\n",
        "# Run the code"
      ],
      "metadata": {
        "id": "LKcdtcwx8vdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Kaleb-Huneau/GAN-Group6.git"
      ],
      "metadata": {
        "id": "vMTlffe6Oou0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1970e5-b021-4776-fe7f-b4a2c16184b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GAN-Group6'...\n",
            "remote: Enumerating objects: 6769, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 6769 (delta 7), reused 26 (delta 7), pack-reused 6743\u001b[K\n",
            "Receiving objects: 100% (6769/6769), 159.34 MiB | 21.69 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "Updating files: 100% (7032/7032), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ua2PwQB_X-ON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Kaggle](https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset/data)"
      ],
      "metadata": {
        "id": "owFM6k6oYDDX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7le-Ad8QBFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing: for the brain MRI dataset [Brain tumor MRI](https://github.com/masoudnick/Brain-Tumor-MRI-Classification/blob/main/Preprocessing.py)\n"
      ],
      "metadata": {
        "id": "iOPMrqb4G1zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "import imutils\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_directory(PATH):\n",
        "\tif not os.path.exists(PATH):\n",
        "\t\tos.mkdir(PATH)\n",
        "\n",
        "def crop_img(img):\n",
        "\t\"\"\"\n",
        "\tFinds the extreme points on the image and crops the rectangular out of them\n",
        "\t\"\"\"\n",
        "\tgray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\tgray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
        "\n",
        "\t# threshold the image, then perform a series of erosions +\n",
        "\t# dilations to remove any small regions of noise\n",
        "\tthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
        "\tthresh = cv2.erode(thresh, None, iterations=2)\n",
        "\tthresh = cv2.dilate(thresh, None, iterations=2)\n",
        "\n",
        "\t# find contours in thresholded image, then grab the largest one\n",
        "\tcnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\tcnts = imutils.grab_contours(cnts)\n",
        "\tc = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "\t# find the extreme points\n",
        "\textLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "\textRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "\textTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "\textBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "\tADD_PIXELS = 0\n",
        "\tnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
        "\n",
        "\treturn new_img\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\ttraining = 'GAN-Group6/Dataset/Training'\n",
        "\ttesting = 'GAN-Group6/Dataset/Testing'\n",
        "\n",
        "\tmake_directory(training)\n",
        "\tmake_directory(testing)\n",
        "\n",
        "\tIMG_SIZE = 28\n",
        "\n",
        "\t# Now, list the directories after creating them\n",
        "\ttraining_dir = os.listdir(training)\n",
        "\ttesting_dir = os.listdir(testing)\n",
        "\ttumors = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "\n",
        "\tfor dir in tumors:\n",
        "\t\t\tsave_path = 'cleaned4/Training/' + dir\n",
        "\t\t\tpath = os.path.join(training, dir)\n",
        "\t\t\timage_dir = os.listdir(path)\n",
        "\t\t\tfor img in image_dir:\n",
        "\t\t\t\t\timage = cv2.imread(os.path.join(path, img))\n",
        "\t\t\t\t\tnew_img = crop_img(image)\n",
        "\t\t\t\t\tnew_img = cv2.resize(new_img, (IMG_SIZE, IMG_SIZE))\n",
        "\t\t \t\t\t#new\n",
        "\t\t\t\t\timgnp = np.asarray(new_img)\n",
        "\t\t\t\t\tnew_imgnp = imgnp[:, :, 0]\n",
        "\n",
        "\t\t\t\t\tnew_imgnp = np.array(new_imgnp)\n",
        "\t\t\t\t\tnew_imgnp = new_imgnp.reshape(28,28,1)\n",
        "\t\t\t\t\t#new\n",
        "\t\t\t\t\tif not os.path.exists(save_path):\n",
        "\t\t\t\t\t\t\tos.makedirs(save_path)\n",
        "\t\t\t\t\tcv2.imwrite(os.path.join(save_path, img), new_imgnp)\n",
        "\n",
        "\tfor dir in tumors:\n",
        "\t\t\tsave_path = 'cleaned4/Testing/' + dir\n",
        "\t\t\tpath = os.path.join(testing, dir)\n",
        "\t\t\timage_dir = os.listdir(path)\n",
        "\t\t\tfor img in image_dir:\n",
        "\t\t\t\t\timage = cv2.imread(os.path.join(path, img))\n",
        "\t\t\t\t\tnew_img = crop_img(image)\n",
        "\t\t\t\t\tnew_img = cv2.resize(new_img, (IMG_SIZE, IMG_SIZE))\n",
        "\t\t \t\t\t#new\n",
        "\t\t\t\t\timgnp = np.asarray(new_img)\n",
        "\t\t\t\t\tnew_imgnp = imgnp[:, :, 0]\n",
        "\n",
        "\t\t\t\t\tnew_imgnp = np.array(new_imgnp)\n",
        "\t\t\t\t\tnew_imgnp = new_imgnp.reshape(28,28,1)\n",
        "\t\t\t\t\t#new\n",
        "\t\t\t\t\tif not os.path.exists(save_path):\n",
        "\t\t\t\t\t\t\tos.makedirs(save_path)\n",
        "\t\t\t\t\tcv2.imwrite(os.path.join(save_path, img), new_imgnp)\n"
      ],
      "metadata": {
        "id": "s-vPtX0IGxqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing to see if alterations work"
      ],
      "metadata": {
        "id": "0rK2I8q2ESOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#img2 = np.asarray(plt.imread(\"cleaned2/Testing/notumor/Tr-noTr_0003.jpg\", format='jpg'))\n",
        "\n",
        "'''\n",
        "new_img = []\n",
        "for i in range(28):\n",
        "  for j in range(28):\n",
        "    new_img.append(img[i][j][0])\n",
        "\n",
        "new_img = img[:, :, 0]\n",
        "\n",
        "new_img = np.array(new_img)\n",
        "new_img = new_img.reshape(28,28,1)\n",
        "plt.imshow(new_img, cmap='gray')\n",
        "'''\n",
        "\n",
        "#img2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "VxADjt9eER0H",
        "outputId": "17fc7868-a993-4812-ec0a-e3cf9dcb53d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'cleaned2/Testing/notumor/Tr-noTr_0003.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-92cb885f783e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cleaned2/Testing/notumor/Tr-noTr_0003.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m '''\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0;34m\"``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m             )\n\u001b[0;32m-> 1563\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1565\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cleaned2/Testing/notumor/Tr-noTr_0003.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Justins code:"
      ],
      "metadata": {
        "id": "nvShI0YFGzgQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3UjyGGOemqe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "#import tensorflow_probability as tfp\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, \\\n",
        "    LeakyReLU, Conv2DTranspose, Conv2D, Dropout, \\\n",
        "        Flatten, Reshape, ReLU, Input, Concatenate\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.python.data import Iterator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import sys\n",
        "import multiprocessing\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "#import stacked_mnist\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "#tfd = tfp.distributions\n",
        "\n",
        "def make_directory(PATH):\n",
        "    if not os.path.exists(PATH):\n",
        "            os.mkdir(PATH)\n",
        "\n",
        "\n",
        "class AlphaGAN(object):\n",
        "    def __init__(self, opt):\n",
        "        self.opt = opt\n",
        "        self.batch_size = 1\n",
        "        self.noise_dim = 28*28\n",
        "        self.epsilon = 1e-8\n",
        "        self.alpha_d = float(opt.alpha_d)\n",
        "        self.alpha_g = float(opt.alpha_g)\n",
        "        self.seed = opt.seed\n",
        "        self.loss_type = opt.loss_type\n",
        "        self.dataset = opt.dataset\n",
        "        self.n_epochs = opt.n_epochs\n",
        "        self.gp = opt.gp\n",
        "        self.scores = np.zeros(self.n_epochs)\n",
        "        self.num_images = opt.num_images\n",
        "        self.gp_coef = opt.gp_coef\n",
        "        if self.dataset != 'cifar10':\n",
        "            self.num_images = 10\n",
        "        self.d_opt = Adam(2e-4, beta_1 = 0.5)\n",
        "        self.g_opt = Adam(2e-4, beta_1 = 0.5)\n",
        "        if self.dataset == 'cifar10':\n",
        "            self.noise_dim = 100\n",
        "        self.l1 = opt.l1\n",
        "        tf.random.set_seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "        if self.dataset == 'mnist':\n",
        "            (self.train_img, _), (self.test_img, _) = tf.keras.datasets.mnist.load_data()\n",
        "            self.train_img = self.train_img.reshape(self.train_img.shape[0], 28, 28, 1)\n",
        "            self.test_img = self.test_img.reshape(self.test_img.shape[0], 28, 28, 1)\n",
        "\n",
        "        elif self.dataset == 'cifar10':\n",
        "            (self.train_img, _), (self.test_img, _) = tf.keras.datasets.cifar10.load_data()\n",
        "            self.train_img = self.train_img.reshape(self.train_img.shape[0], 32, 32, 3)\n",
        "            self.test_img = self.test_img.reshape(self.test_img.shape[0], 32, 32, 3)\n",
        "\n",
        "        elif self.dataset == 'stacked-mnist':\n",
        "            (self.train_img, _), (self.test_img, _) = stacked_mnist.load_data()\n",
        "            self.train_img = self.train_img.reshape(self.train_img.shape[0], 32, 32, 3)\n",
        "            self.test_img = self.test_img.reshape(self.test_img.shape[0], 32, 32, 3)\n",
        "\n",
        "        elif self.dataset == 'mri':\n",
        "            # Iterate over all files in the tumor directory\n",
        "\n",
        "            tumor_dir = '/content/cleaned4/Training/notumor/'\n",
        "            tumor_training_images = []\n",
        "            for filename in os.listdir(tumor_dir):\n",
        "                if filename.endswith('.jpg'):  # Assuming images are in JPG format\n",
        "                    image_path = os.path.join(tumor_dir, filename)\n",
        "                    image = plt.imread(image_path, format='jpg')\n",
        "                    tumor_training_images.append(image)\n",
        "\n",
        "            # Convert the list of images to a NumPy array\n",
        "            self.train_img = np.asarray(tumor_training_images)\n",
        "\n",
        "            tumor_dir = '/content/cleaned4/Testing/notumor/'\n",
        "            tumor_testing_images = []\n",
        "            for filename in os.listdir(tumor_dir):\n",
        "                if filename.endswith('.jpg'):  # Assuming images are in JPG format\n",
        "                    image_path = os.path.join(tumor_dir, filename)\n",
        "                    image = plt.imread(image_path, format='jpg')\n",
        "                    tumor_testing_images.append(image)\n",
        "\n",
        "            # Convert the list of images to a NumPy array\n",
        "            self.test_img = np.asarray(tumor_testing_images)\n",
        "\n",
        "            self.train_img = self.train_img.reshape(self.train_img.shape[0], 28, 28, 1)\n",
        "            self.test_img = self.test_img.reshape(self.test_img.shape[0], 28, 28, 1)\n",
        "\n",
        "          # self.train_img =  np.asarray([plt.imread(\"cleaned4/Training/notumor/Tr-noTr_0000.jpg\", format='jpg')])\n",
        "          # self.test_img = np.asarray([plt.imread(\"cleaned4/Testing/notumor/Te-noTr_0000.jpg\", format='jpg')])\n",
        "\n",
        "        self.real_mu, self.real_sigma = self.get_eval_metrics(self.train_img)\n",
        "        self.train_data, self.test_data = self.clean_data(self.train_img, train = True), self.clean_data(self.test_img, train = False)\n",
        "\n",
        "\n",
        "    def get_eval_metrics(self, data):\n",
        "        img_dims = data.shape\n",
        "        eval_img = data[np.random.choice(img_dims[0], 1, replace=False), :, :, :]\n",
        "        eval_img = eval_img.reshape(1, np.prod(img_dims[1:])).astype('float32') # CHANGE BACK TO 10000\n",
        "        eval_img = eval_img / 255.0\n",
        "        real_mu = eval_img.mean(axis = 0)\n",
        "        eval_img = np.transpose(eval_img)\n",
        "        real_sigma = np.cov(eval_img)\n",
        "        return real_mu, real_sigma\n",
        "\n",
        "    def clean_data(self, data, train):\n",
        "\n",
        "        new_data = data.astype('float32')\n",
        "        new_data = (new_data - 127.5) / 127.5\n",
        "        if train:\n",
        "            new_data = tf.data.Dataset.from_tensor_slices(new_data)\n",
        "            return new_data.shuffle(100000).batch(self.batch_size)\n",
        "        return new_data\n",
        "\n",
        "    def gen_loss_vanilla(self):\n",
        "        bce = tf.keras.losses.BinaryCrossentropy(from_logits = False)\n",
        "        loss_expr =  bce(tf.ones_like(self.fake_predicted_labels), self.fake_predicted_labels)\n",
        "        if self.l1:\n",
        "            loss_expr = tf.math.abs(loss_expr - (-tf.math.log(2.0)))\n",
        "        return loss_expr\n",
        "\n",
        "    def gen_loss_vanilla_l1(self):\n",
        "        return tf.math.abs(self.gen_loss_vanilla() - (-tf.math.log(2.0)))\n",
        "\n",
        "\n",
        "    def gen_loss_alpha(self):\n",
        "        fake_expr = tf.math.pow(1 - self.fake_predicted_labels, ((self.alpha_g-1)/self.alpha_g)*tf.ones_like(self.fake_predicted_labels))\n",
        "        fake_loss = tf.math.reduce_mean(fake_expr)\n",
        "        loss_expr = (self.alpha_g/(self.alpha_g - 1))*(fake_loss - 2.0)\n",
        "        if self.l1:\n",
        "            equil_val = (self.alpha_g)/(self.alpha_g - 1)*(tf.math.pow(2.0, 1/self.alpha_g) - 2)\n",
        "            loss_expr = tf.math.abs(loss_expr - equil_val)\n",
        "        return loss_expr\n",
        "\n",
        "    def dis_loss_vanilla(self):\n",
        "        bce = tf.keras.losses.BinaryCrossentropy(from_logits = False)\n",
        "        real_loss = bce(tf.ones_like(self.real_predicted_labels), self.real_predicted_labels)\n",
        "        fake_loss = bce(tf.zeros_like(self.fake_predicted_labels), self.fake_predicted_labels)\n",
        "        r1_penalty = 0\n",
        "        if self.gp:\n",
        "            gradients = tf.gradients(-tf.math.log(1 / self.real_predicted_labels - 1), [self.img])[0]\n",
        "            r1_penalty = tf.reduce_mean(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
        "        return real_loss + fake_loss + self.gp_coef*r1_penalty\n",
        "\n",
        "    def dis_loss_alpha(self):\n",
        "        real_expr = tf.math.pow(self.real_predicted_labels, ((self.alpha_d-1)/self.alpha_d)*tf.ones_like(self.real_predicted_labels))\n",
        "        real_loss = tf.math.reduce_mean(real_expr)\n",
        "        fake_expr = tf.math.pow(1 - self.fake_predicted_labels, ((self.alpha_d-1)/self.alpha_d)*tf.ones_like(self.fake_predicted_labels))\n",
        "        fake_loss = tf.math.reduce_mean(fake_expr)\n",
        "        r1_penalty = 0\n",
        "        if self.gp:\n",
        "            gradients = tf.gradients(-tf.math.log(1 / self.real_predicted_labels - 1), [self.img])[0]\n",
        "            r1_penalty = tf.reduce_mean(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
        "        loss_expr = -(self.alpha_d/(self.alpha_d - 1))*(real_loss + fake_loss - 2.0)\n",
        "\n",
        "\n",
        "        return loss_expr + self.gp_coef*r1_penalty\n",
        "\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if self.dataset == 'mnist' or self.dataset == 'mri':\n",
        "\n",
        "            model.add(Dense(7 * 7 * 256, use_bias=False, kernel_initializer=\n",
        "            RandomNormal(mean=0.0, stddev=0.01), input_shape=(self.noise_dim,)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(LeakyReLU())\n",
        "\n",
        "            model.add(Reshape((7, 7, 256)))\n",
        "\n",
        "\n",
        "            model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False, kernel_initializer=\n",
        "            RandomNormal(mean=0.0, stddev=0.01)))\n",
        "\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(LeakyReLU())\n",
        "\n",
        "            model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=\n",
        "            RandomNormal(mean=0.0, stddev=0.01)))\n",
        "\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(LeakyReLU())\n",
        "\n",
        "            model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', activation='tanh', use_bias=False,\n",
        "                                        kernel_initializer=RandomNormal(mean=0.0, stddev=0.01)))\n",
        "        return model\n",
        "\n",
        "\n",
        "    def build_dq(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        if self.dataset == 'mnist' or self.dataset == 'mri':\n",
        "            model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', kernel_initializer=\n",
        "            RandomNormal(mean=0.0, stddev=0.01)))\n",
        "            model.add(LeakyReLU())\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "            model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same', kernel_initializer=\n",
        "            RandomNormal(mean=0.0, stddev=0.01)))\n",
        "            model.add(LeakyReLU())\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "            model.add(Flatten())\n",
        "            model.add(Dense(1, activation='sigmoid', kernel_initializer=\n",
        "            RandomNormal(mean=0.0, stddev=0.01)))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def build_gan(self):\n",
        "        self.generator = self.build_generator()\n",
        "        self.discriminator = self.build_dq()\n",
        "        self.generator_loss = self.gen_loss_alpha\n",
        "        self.discriminator_loss = self.dis_loss_alpha\n",
        "        if self.alpha_d == 1.0:\n",
        "            self.discriminator_loss = self.dis_loss_vanilla\n",
        "        if self.alpha_g == 1.0:\n",
        "            self.generator_loss = self.gen_loss_vanilla\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, real_images):\n",
        "\n",
        "        z = tf.random.normal([self.batch_size, self.noise_dim])\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape, tf.GradientTape() as q_tape:\n",
        "            self.discriminator.trainable = True\n",
        "            self.img = real_images\n",
        "            self.real_predicted_labels = self.discriminator(real_images, training = True)\n",
        "\n",
        "            self.generated_images = self.generator(z, training = True)\n",
        "            self.fake_predicted_labels = self.discriminator(self.generated_images, training = True)\n",
        "\n",
        "            self.dis_loss_value = self.discriminator_loss()\n",
        "            self.gen_loss_value = self.generator_loss()\n",
        "\n",
        "        dis_gradients = dis_tape.gradient(self.dis_loss_value, self.discriminator.trainable_variables)\n",
        "        self.d_opt.apply_gradients(zip(dis_gradients, self.discriminator.trainable_variables))\n",
        "        self.discriminator.trainable = False\n",
        "        gen_gradients = gen_tape.gradient(self.gen_loss_value, self.generator.trainable_variables)\n",
        "        self.g_opt.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n",
        "\n",
        "        return self.dis_loss_value, self.gen_loss_value\n",
        "\n",
        "    def build_directory(self):\n",
        "        gan_name = 'AlphaGAN'\n",
        "        if self.alpha_d == 1.0 and self.alpha_g == 1.0:\n",
        "            gan_name = 'VanillaGAN'\n",
        "        '''\n",
        "        SEEDS = [123, 1600, 60677, 15859, 79878]\n",
        "        if self.dataset == 'mnist':\n",
        "            SEEDS = [123, 500, 1600, 199621, 60677, 20435, 15859, 33764, 79878, 36123]\n",
        "        '''\n",
        "        make_directory(gan_name)\n",
        "        make_directory(f'{gan_name}/{self.dataset}')\n",
        "        if gan_name == 'AlphaGAN':\n",
        "            make_directory(f'{gan_name}/{self.dataset}/alpha-d{self.alpha_d}-g{self.alpha_g}')\n",
        "\n",
        "        subfolders = [f[0] for f in os.walk(f'AlphaGAN/{self.dataset}/alpha-d{self.alpha_d}-g{self.alpha_g}')]\n",
        "        folders = [f for f in subfolders if f.startswith(f'AlphaGAN/{self.dataset}/alpha-d{self.alpha_d}-g{self.alpha_g}/v')]\n",
        "\n",
        "        versions = [f.split('/v')[1] for f in folders]\n",
        "        versions = [int(v) for v in versions if v.isnumeric()]\n",
        "        version = 1\n",
        "        if versions:\n",
        "            version = max(versions) + 1\n",
        "        folder_created = False\n",
        "\n",
        "        while not folder_created:\n",
        "            self.path = f'AlphaGAN/{self.dataset}/alpha-d{self.alpha_d}-g{self.alpha_g}/v'+str(version)\n",
        "\n",
        "            try:\n",
        "                make_directory(self.path)\n",
        "                folder_created = True\n",
        "            except:\n",
        "                version += 1\n",
        "        '''\n",
        "\n",
        "        version = SEEDS.index(self.seed) + 1\n",
        "        if self.gp and self.l1:\n",
        "            version = version + 15 if self.dataset != 'mnist' else version + 30\n",
        "        elif self.gp:\n",
        "            version = version + 5 if self.dataset != 'mnist' else version + 10\n",
        "        elif self.l1:\n",
        "            version = version + 10 if self.dataset != 'mnist' else version + 20\n",
        "        '''\n",
        "        self.path = f'{gan_name}/{self.dataset}/alpha-d{self.alpha_d}-g{self.alpha_g}/v'+str(version)\n",
        "        if gan_name == 'VanillaGAN':\n",
        "            self.path = f'{gan_name}/{self.dataset}/v'+str(version)\n",
        "\n",
        "        make_directory(self.path)\n",
        "        make_directory(self.path + '/metrics')\n",
        "        make_directory(self.path + '/metrics/accuracy')\n",
        "        make_directory(self.path + '/metrics/losses')\n",
        "        make_directory(self.path + '/img')\n",
        "        make_directory(self.path + '/models')\n",
        "\n",
        "        with open(self.path+'/description.txt', 'w') as f:\n",
        "            f.write(f'version={version}\\n')\n",
        "            for k, v in vars(self.opt).items():\n",
        "                f.write(f'{k}={v}')\n",
        "                f.write('\\n')\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        self.get_data()\n",
        "        self.build_gan()\n",
        "        self.build_directory()\n",
        "        gen_loss_history = np.zeros(self.n_epochs)\n",
        "        dis_loss_history = np.zeros(self.n_epochs)\n",
        "        epoch_times = []\n",
        "        img_times = []\n",
        "        epochs_passed = 0\n",
        "        for epoch in range(1, self.n_epochs + 1):\n",
        "            print(f\"Epoch {epoch}\")\n",
        "            n_batches = 0\n",
        "            start_epoch = time.time()\n",
        "            for real_images in iter(self.train_data):\n",
        "\n",
        "                dis_loss_value, gen_loss_value = self.train_step(real_images)\n",
        "\n",
        "                gen_loss_history[epoch - 1] += gen_loss_value\n",
        "                dis_loss_history[epoch - 1] += dis_loss_value\n",
        "\n",
        "                n_batches += 1\n",
        "            gen_loss_history = gen_loss_history/n_batches\n",
        "            dis_loss_history = dis_loss_history/n_batches\n",
        "            end_epoch = time.time()\n",
        "            epoch_times.append(end_epoch - start_epoch)\n",
        "            # self.evaluate(epoch) Can't find an evalate function\n",
        "            start_img = time.time()\n",
        "            self.save_generated_images(epoch)\n",
        "            end_img = time.time()\n",
        "            img_times.append(end_img - start_img)\n",
        "            epochs_passed += 1\n",
        "            # try:\n",
        "            #      self.scores[epoch - 1] = self.compute_fid()\n",
        "            # except Exception as e:\n",
        "            #      print(str(e))\n",
        "            #      break\n",
        "\n",
        "\n",
        "        np.save(self.path + '/metrics/losses/gen_loss.npy', gen_loss_history)\n",
        "        np.save(self.path + '/metrics/losses/dis_loss.npy', dis_loss_history)\n",
        "\n",
        "        self.generator.save(self.path+ '/models/generator')\n",
        "        self.discriminator.save(self.path + '/models/discriminator')\n",
        "\n",
        "        time_df = pd.DataFrame({'epoch':list(range(1, epochs_passed + 1)),\n",
        "        'epoch_time':epoch_times, 'img_times':img_times})\n",
        "\n",
        "        time_df.to_pickle(self.path+'/times.pkl')\n",
        "        np.save(self.path + '/scores.npy', self.scores)\n",
        "        if epochs_passed == self.n_epochs:\n",
        "            for epoch in range(epochs_passed):\n",
        "                if epoch != np.nanargmin(self.scores):\n",
        "                    os.remove(self.path + '/img/predictions' + str(epoch + 1) + \".npy\")\n",
        "\n",
        "\n",
        "\n",
        "    def compute_fid(self):\n",
        "        fake_images = self.generator(tf.random.normal([10000, self.noise_dim]))\n",
        "        fake_images = fake_images.numpy()\n",
        "        if self.dataset == 'mnist' or self.dataset == 'mri':\n",
        "            fake_images = fake_images.reshape(10000, 28*28)\n",
        "        fake_images = (fake_images * 127.5 + 127.5) / 255.0\n",
        "        fake_mu = fake_images.mean(axis=0)\n",
        "        fake_sigma = np.cov(np.transpose(fake_images))\n",
        "        covSqrt = sp.linalg.sqrtm(np.matmul(fake_sigma, self.real_sigma))\n",
        "        if np.iscomplexobj(covSqrt):\n",
        "            covSqrt = covSqrt.real\n",
        "        fidScore = np.linalg.norm(self.real_mu - fake_mu) + np.trace(self.real_sigma + fake_sigma - 2 * covSqrt)\n",
        "        return fidScore\n",
        "\n",
        "\n",
        "\n",
        "    def save_generated_images(self, epoch):\n",
        "        if epoch == 1:\n",
        "            self.z_eval = tf.random.normal([self.num_images, self.noise_dim])\n",
        "\n",
        "\n",
        "        imgs = self.generator(self.z_eval, training = False)\n",
        "        print(imgs.shape)\n",
        "\n",
        "        np.save(self.path+'/img/predictions' + str(epoch) + '.npy', imgs)\n",
        "        return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgzhqre-e707",
        "outputId": "f1c37dfc-74c0-4b57-9b3e-1fda2a5db4bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-123-163ae7363b09>:118: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  real_sigma = np.cov(eval_img)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2704: RuntimeWarning: divide by zero encountered in divide\n",
            "  c *= np.true_divide(1, fact)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2704: RuntimeWarning: invalid value encountered in multiply\n",
            "  c *= np.true_divide(1, fact)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "(10, 28, 28, 1)\n",
            "Epoch 2\n",
            "(10, 28, 28, 1)\n",
            "Epoch 3\n",
            "(10, 28, 28, 1)\n",
            "Epoch 4\n",
            "(10, 28, 28, 1)\n",
            "Epoch 5\n",
            "(10, 28, 28, 1)\n",
            "Epoch 6\n",
            "(10, 28, 28, 1)\n",
            "Epoch 7\n",
            "(10, 28, 28, 1)\n",
            "Epoch 8\n",
            "(10, 28, 28, 1)\n",
            "Epoch 9\n",
            "(10, 28, 28, 1)\n",
            "Epoch 10\n",
            "(10, 28, 28, 1)\n",
            "Epoch 11\n",
            "(10, 28, 28, 1)\n",
            "Epoch 12\n",
            "(10, 28, 28, 1)\n",
            "Epoch 13\n",
            "(10, 28, 28, 1)\n",
            "Epoch 14\n",
            "(10, 28, 28, 1)\n",
            "Epoch 15\n",
            "(10, 28, 28, 1)\n",
            "Epoch 16\n",
            "(10, 28, 28, 1)\n",
            "Epoch 17\n",
            "(10, 28, 28, 1)\n",
            "Epoch 18\n",
            "(10, 28, 28, 1)\n",
            "Epoch 19\n",
            "(10, 28, 28, 1)\n",
            "Epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 28, 28, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "#from keras.datasets import mnist\n",
        "import gc\n",
        "import argparse\n",
        "\n",
        "class Option():\n",
        "    \"\"\"\n",
        "    Empty class to hold the gan options\n",
        "    \"\"\"\n",
        "    def __init__(self, gan_type, alpha, seed, c_type, n_epochs, dataset, loss_type, lambda_d, lambda_c, num_images, gp, gen_lr, dis_lr, q_lr, gp_coef, alpha_d, alpha_g, k, shifted, l1):\n",
        "        self.gan_type = gan_type\n",
        "        self.alpha = alpha\n",
        "        self.seed = seed\n",
        "        self.c_type = c_type\n",
        "        self.n_epochs = n_epochs\n",
        "        self.dataset = dataset\n",
        "        self.loss_type = loss_type\n",
        "        self.lambda_d = lambda_d\n",
        "        self.lambda_c = lambda_c\n",
        "        self.num_images = num_images\n",
        "        self.gp = gp\n",
        "        self.gen_lr = gen_lr\n",
        "        self.dis_lr = dis_lr\n",
        "        self.q_lr = q_lr\n",
        "        self.gp_coef = gp_coef\n",
        "        self.alpha_d = alpha_d\n",
        "        self.alpha_g = alpha_g\n",
        "        self.k = k\n",
        "        self.shifted = shifted\n",
        "        self.l1 = l1\n",
        "        return\n",
        "#set up options\n",
        "opts = Option(gan_type='alpha', alpha=3.0, seed=42, c_type='discrete', n_epochs=20, dataset='mri', loss_type='vanilla', lambda_d=1.0, lambda_c=0.1, num_images= 100, gp=False, gen_lr=0.0002, dis_lr=0.0002, q_lr=0.0002, gp_coef=5.0, alpha_d=3.0, alpha_g=3.0, k=2.0, shifted=False, l1=False)\n",
        "\n",
        "# Define an alphagan to test\n",
        "gan = AlphaGAN(opts)\n",
        "\n",
        "# sets data to mnist and configures it for the gan\n",
        "gan.dataset = 'mri'\n",
        "gan.get_data()\n",
        "\n",
        "\n",
        "#build the generative network\n",
        "gan.build_gan()\n",
        "gan.train()\n",
        "\n",
        "\n",
        "\n",
        "# while testing not going to include fid computations\n",
        "# gan.compute_fid()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print the results\n"
      ],
      "metadata": {
        "id": "XjxLEGkNWfXW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auYhEuVnfrG7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "a7f51fe2-e67a-41ca-abc3-aa067a5ea831"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAIi0lEQVR4nO3cMYpUyx7A4dPPUWHA0MQ9CObiRgQDA3di7AJMDAxciLgVNyCKiNA3GPjBvV7eO+c8uyyP3xdPzenq6p4fFcz/dD6fzwsALMvyn1/9AgCYhygAEFEAIKIAQEQBgIgCABEFACIKAOTqkr/8dDpd8tcDsMGa/1V2UwAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCAFk9EG/PcLs1w5d+xnOWZVnevn27ec2zZ882r/n27dvmNXfu3Nm8Zs97tyzL8vz5881r3rx5s3nNnnP68uXL5jXLsizX19e71jHOyO86l+WmAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAcjqvnGRleBXwM+0d+jjz36KRe9r7rP/FTQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgpqQB/iDV/7t0UAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBArtb+4Mq5eX9jiB78fo74XT/ini7FTQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAGT1QLw/dTjUn+D9+/eb1zx58uQCr4T/ZtRQtz1r9ry2vUbtaY+978NMf1/dFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQE7nlROcZhrYBPz+Rg6PGzVMcPY9reGmAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAcrX2B+/evbv5l3/9+nXzmtkH740arDXSpQZr/dPe92HU69tj5NnOfE4jz+iI38Hbt29vXnOpc3JTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAcjpfcLzh7JMJGWvvR23mz9ERJ3ba043Z97SHKakAbCIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQq7U/eMThUNwYNSxs5GfInm7M/L0dOSBxz5oLzgr9wUzn5KYAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQCyeiAexzXTMK6fxZ7m9+DBg2HPGjVMcOQQvUs9y00BgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBk9UC8UYOeZh/6NWqw1khHPFt72m/2QXB7jDqn+/fv71p369atzWsudU5uCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQE7nleMN3717t/mXP336dPMajmvvJM2Zp8wecWruETmnG6akArCJKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQFYPxJt9ONTjx483r/nw4cMFXsnvZ9SwsJED8Y44AG32cxpl1DmNfB9m2pObAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAyGEG4gE/x+yD92YekDj7ntZwUwAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCALla+4OXGr70T7MP3hs1WGuk169fb17z4sWLzWv2vg+jPnt7jDzbmb+DI89o1Hfw0aNHm9fsNdPAPjcFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQ0/mCk6xmHwQH/68jDkg8Iud0w0A8ADYRBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkKu1P3jEiYHcGDVB8tOnT5vXLMuy3Lt3b/OaI07FPNqe9g5o3rOnPWsuOED6BzOdk5sCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADI6oF4HNeoYVx7BtvtNWpPr169GvKcZRm3p6MN3luWcXsaOUTvUs9yUwAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCADmdR05wWsFgrfFGfQRGvg8z72myr9wP7OnGyD3NNOzQTQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAGT1QLzbt29v/uXfv3/fvAZ+J0cckHhERzynSw3sc1MAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgBZPRBv9uFQR7N32NWecxo1LMye9j9n5LNmfs7eZ+1xqYFz/2amPbkpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAufrVL4Bfb9SExpGTdo841XfUnh4+fDjkOSPNPvl1j0s9y00BgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgDkdB45wWmFkYPMrq+vN6/5/PnzBV7Jj0a+D6M+Anv3NNlH9G9mH5q2xxH3dESXOic3BQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkNUD8UYOaGP/gLGZz8mebsy8n6NyTjcMxANgE1EAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYBc/eoX8LOMGnh1xMFao/Y08n1wTjdm/oyPHJA48vXtMdNnz00BgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADIYaakzjRlkH83cirmx48fdz1rZj7j+80++XWPSz3LTQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAGT1QLxRg56OOPRrz3v38uXLYc8a9Zy9ZzvznmZ+zl5HHAR3xL8rlzonNwUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoAJDTefbpXAAM46YAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAED+AtzGAt8t0FuIAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "filename = \"/content/AlphaGAN/mri/alpha-d3.0-g3.0/v30/img/predictions1.npy\"\n",
        "\n",
        "# Load the numpy file\n",
        "img_array = np.load(filename, allow_pickle=True)\n",
        "\n",
        "# Normalize the data to [0, 1]\n",
        "img_array_normalized = (img_array - img_array.min()) / (img_array.max() - img_array.min())\n",
        "\n",
        "# Add the channel dimension back\n",
        "img_array_normalized_with_channel = img_array_normalized[0, :, :, 0]\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(img_array_normalized_with_channel, cmap=\"gray\")\n",
        "plt.axis('off')  # Turn off axis labels\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLxlDylfmQMK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}